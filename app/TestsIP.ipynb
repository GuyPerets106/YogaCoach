{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/guyperets/Downloads/example_adva_full.mov\"\n",
    "SMALL_CLIP_PATH = \"/Users/guyperets/Downloads/SmallClip.mp4\"\n",
    "# Background Subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=50, detectShadows=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started IP...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(PATH)\n",
    "i = 0\n",
    "# ? Define a ROI\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        i += 1\n",
    "        if i < 200:\n",
    "            continue\n",
    "        if i == 200:\n",
    "            print(\"Started IP...\")\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(frame, (3, 3), 0)\n",
    "        canny = cv2.Canny(blurred, 80, 240)\n",
    "        backsub = backSub.apply(canny)\n",
    "        # _, binary = cv2.threshold(backsub, 100, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        dilated = cv2.dilate(backsub, (5, 5), iterations=10)\n",
    "        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        mask = np.zeros_like(frame)\n",
    "        centroids = []\n",
    "        for cnt in contours:\n",
    "            # Connect the contours\n",
    "            cv2.drawContours(mask, [cnt], -1, (255, 255, 255), cv2.FILLED)\n",
    "            M = cv2.moments(cnt)\n",
    "            if M['m00'] != 0:\n",
    "                cX = int(M['m10'] / M['m00'])\n",
    "                cY = int(M['m01'] / M['m00'])\n",
    "                centroids.append((cX, cY))\n",
    "        # close_centroids = []\n",
    "        # for c1 in centroids:\n",
    "        #     for c2 in centroids:\n",
    "        #         if c1 != c2:\n",
    "        #             if abs(c1[0] - c2[0]) > 20 and abs(c1[1] - c2[1]) < 20:\n",
    "        #                 close_centroids.append((c1, c2))\n",
    "        # for close_pair in close_centroids:\n",
    "        #     cv2.fillPoly(mask, np.array([close_pair]), (255, 255, 255))\n",
    "            \n",
    "        cv2.imshow(\"BackSub\", mask)\n",
    "        key = cv2.waitKey(0)\n",
    "        if key & 0xFF == 27 or key & 0xFF == ord('q'): # ESC or 'q'\n",
    "            break\n",
    "    else:\n",
    "        print(\"Video ended\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(PATH)\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video ended or failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "\n",
    "    # Threshold the foreground mask to get a binary image\n",
    "    _, binary = cv2.threshold(fgMask, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create an empty mask with the same size as the frame\n",
    "    mask = np.zeros_like(frame, dtype=np.uint8)\n",
    "\n",
    "    # Filter and draw contours on the mask\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 500:  # Filter small contours based on area\n",
    "            cv2.drawContours(mask, [cnt], -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Convert the mask to grayscale and apply threshold to ensure binary mask\n",
    "    mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask_binary = cv2.threshold(mask_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Display the original frame and the mask\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Mask\", mask_binary)\n",
    "\n",
    "    # Wait for a key press and break the loop if 'q' or 'ESC' is pressed\n",
    "    key = cv2.waitKey(30) & 0xFF\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_object(roi):\n",
    "    # Initialize the Kalman filter\n",
    "    kf = cv2.KalmanFilter(4, 2, 0)\n",
    "    kf.measurementMatrix = np.array([[1, 0], [0, 1]])\n",
    "    kf.processNoiseCov = np.array([[0.1, 0], [0, 0.1]])\n",
    "    kf.measurementNoiseCov = np.array([[0.01, 0], [0, 0.01]])\n",
    "\n",
    "    # Get the current measurement of the object\n",
    "    x, y = (roi.shape[1] // 2, roi.shape[0] // 2)\n",
    "    measurement = np.array([x, y])\n",
    "\n",
    "    # Update the Kalman filter with the current measurement\n",
    "    kf.predict()\n",
    "    kf.update(measurement)\n",
    "\n",
    "    # Predict the next state of the object\n",
    "    prediction = kf.predict()\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_tracker(joint_list):\n",
    "    multi_tracker = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "    bboxes = []\n",
    "    colors = [] \n",
    "    \n",
    "    # OpenCV's selectROI function doesn't work for selecting multiple objects in Python\n",
    "    # So we will call this function in a loop till we are done selecting all objects\n",
    "    for i in range(len(joint_list)):\n",
    "        # draw bounding boxes over objects\n",
    "        # selectROI's default behaviour is to draw box starting from the center\n",
    "        # when fromCenter is set to false, you can draw box starting from top left corner\n",
    "        bbox = cv2.selectROI('MultiTracker', frame)\n",
    "        bboxes.append(bbox)\n",
    "        colors.append((randint(0, 255), randint(0, 255), randint(0, 255)))\n",
    "        print(\"Press q to quit selecting boxes and start tracking\")\n",
    "        print(\"Press any other key to select next object\")\n",
    "\n",
    "\n",
    "    print(f'Selected bounding boxes: {bboxes}')\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        multi_tracker.add(cv2.legacy.TrackerMOSSE_create(), frame, bbox)\n",
    "        \n",
    "    \n",
    "    return multi_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Normalize the frame to be in the range [0, 1]\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Joint Tracking (Using OpenCV Built-In Trackers)\n",
    "## Currently Working HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame shape: (720, 1280, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from random import randint\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "sleep(.5)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "    sys.exit()\n",
    "height, width, _ = frame.shape\n",
    "print(f'Frame shape: {frame.shape}')\n",
    "w = 70\n",
    "h = 70\n",
    "# joints = {\"head\": (width // 2 - w // 2, height // 4 - h // 2, 100, 100), # (x, y, w, h) - xy is the top left corner\n",
    "#           \"right_elbow\": (width // 2 + w, height // 4 + h, 100, 100),\n",
    "#           \"left_elbow\": (width // 2 - 2 * w, height // 4 + h, 100, 100),\n",
    "#           \"torso\": (width // 2 - w // 2, height // 4 + h, 100, 100),\n",
    "#           \"right_knee\": (width // 2 + int(1.5*w / 2) - 50, height // 4 + 3 * h, 100, 100),\n",
    "#           \"left_knee\": (width // 2 - int(1.5*w / 2) - 50, height // 4 + 3 * h, 100, 100)}\n",
    "\n",
    "joints = {\"shoulder\": (width // 2 - w // 2, height // 5, w, h), # (x, y, w, h) - xy is the top left corner\n",
    "          \"elbow\": (width // 2 + w, height // 5, w, h),\n",
    "          \"wrist\": (width // 2 + 2 * w, height // 5, w, h),\n",
    "          \"hip\": (width // 2 - w // 2, height // 5 + 2 * h, w, h)}\n",
    "        #   \"knee\": (width // 2 - w // 2, height // 5 + 3 * h, w, h),\n",
    "        #   \"ankle\": (width // 2 - w // 2, height // 5 + 4 * h + 50, w, h)}\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "\n",
    "# Create a tracker for each joint\n",
    "trackers = {joint: cv2.legacy.TrackerCSRT_create() for joint in joints.keys()}\n",
    "\n",
    "for joint, tracker in trackers.items():\n",
    "    ret = tracker.init(frame, joints[joint])\n",
    "    if not ret:\n",
    "        print(f\"Failed to initialize tracker for {joint}\")\n",
    "\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=50, detectShadows=False)\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "    if i < 500:\n",
    "        # Apply background subtraction\n",
    "        # fgMask = backSub.apply(frame)\n",
    "        # Show each rectangle in the right position\n",
    "        # cv2.rectangle(frame, joints[\"head\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"right_elbow\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"left_elbow\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"torso\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"right_knee\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"left_knee\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"shoulder\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"elbow\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"wrist\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"hip\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"knee\"], (0, 255, 0), 2)\n",
    "        # cv2.rectangle(frame, joints[\"ankle\"], (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    centroids = {}\n",
    "    # success, boxes = multi_tracker.update(frame)\n",
    "    # print(f'Updated tracker, success = {success}')\n",
    "    # if success:\n",
    "    #     for i, box in enumerate(boxes):\n",
    "    #         print(f'Tracking box {list(joints.keys())[i]}: {box}')\n",
    "    #         x, y, w, h = [int(v) for v in box]\n",
    "    #         center_x = x + w // 2\n",
    "    #         center_y = y + h // 2\n",
    "    #         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #         cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "    #         centroids[list(joints.keys())[i]] = (center_x, center_y)\n",
    "    # else:\n",
    "    #     cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    for joint, tracker in trackers.items():\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "            center_x = x + w // 2\n",
    "            center_y = y + h // 2\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "            centroids[joint] = (center_x, center_y)\n",
    "        else:\n",
    "            cv2.putText(frame, f\"Tracking failure detected for {joint}\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    \n",
    "    \n",
    "    # Connect the centroids with lines\n",
    "    if len(list(centroids.values())) == len(joints.keys()):\n",
    "        cv2.line(frame, centroids[\"shoulder\"], centroids[\"elbow\"], (0, 255, 0), 2)\n",
    "        cv2.line(frame, centroids[\"elbow\"], centroids[\"wrist\"], (0, 255, 0), 2)\n",
    "        # cv2.line(frame, centroids[\"hip\"], centroids[\"knee\"], (0, 255, 0), 2)\n",
    "        # cv2.line(frame, centroids[\"knee\"], centroids[\"ankle\"], (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 16:36:10.008 Python[76304:4961073] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read video file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guyperets/Documents/IP_Course/YogaCoach/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "\n",
    "# Set up tracker.\n",
    "# Instead of MIL, you can also use\n",
    "\n",
    "tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'DaSiamRPN', 'MOSSE', 'CSRT']\n",
    "tracker_type = tracker_types[2]\n",
    "i = 0\n",
    "while i < len(tracker_types):\n",
    "    for tracker_type in tracker_types:\n",
    "        if tracker_type == 'BOOSTING':\n",
    "            tracker = cv2.legacy.TrackerBoosting_create()\n",
    "        # if tracker_type == 'MIL':\n",
    "        #     tracker = cv2.TrackerMIL_create()\n",
    "        # if tracker_type == 'KCF':\n",
    "        #     tracker = cv2.TrackerKCF_create()\n",
    "        elif tracker_type == 'TLD':\n",
    "            tracker = cv2.legacy.TrackerTLD_create()\n",
    "        elif tracker_type == 'MEDIANFLOW':\n",
    "            tracker = cv2.legacy.TrackerMedianFlow_create()\n",
    "        elif tracker_type == 'MOSSE':\n",
    "            tracker = cv2.legacy.TrackerMOSSE_create()\n",
    "        elif tracker_type == \"CSRT\":\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Read video\n",
    "        video = cv2.VideoCapture(0)\n",
    "\n",
    "        # Exit if video not opened.\n",
    "        if not video.isOpened():\n",
    "            print(\"Could not open video\")\n",
    "            sys.exit()\n",
    "\n",
    "        # Read first frame.\n",
    "        ok, frame = video.read()\n",
    "        if not ok:\n",
    "            print('Cannot read video file')\n",
    "            sys.exit()\n",
    "            \n",
    "        # Define an initial bounding box\n",
    "        bbox = (287, 23, 86, 320)\n",
    "\n",
    "        # Uncomment the line below to select a different bounding box\n",
    "        bbox = cv2.selectROI(frame, False)\n",
    "\n",
    "        # Initialize tracker with first frame and bounding box\n",
    "        ok = tracker.init(frame, bbox)\n",
    "\n",
    "        while True:\n",
    "            # Read a new frame\n",
    "            ok, frame = video.read()\n",
    "            if not ok:\n",
    "                break\n",
    "                \n",
    "            # Start timer\n",
    "            timer = cv2.getTickCount()\n",
    "\n",
    "            # Update tracker\n",
    "            ok, bbox = tracker.update(frame)\n",
    "\n",
    "            # Calculate Frames per second (FPS)\n",
    "            fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "\n",
    "            # Draw bounding box\n",
    "            if ok:\n",
    "                # Tracking success\n",
    "                p1 = (int(bbox[0]), int(bbox[1]))\n",
    "                p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "                cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "            else :\n",
    "                # Tracking failure\n",
    "                cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    "\n",
    "            # Display tracker type on frame\n",
    "            cv2.putText(frame, tracker_type + \" Tracker\", (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2)\n",
    "            \n",
    "            # Display FPS on frame\n",
    "            cv2.putText(frame, \"FPS : \" + str(int(fps)), (100,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50), 2)\n",
    "\n",
    "            # Display result\n",
    "            cv2.imshow(\"Tracking\", frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 27 or key == ord('q'):\n",
    "                break\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "height, width, _ = frame.shape\n",
    "joints = {\"head\": (int(width * 0.25), int(height * 0.25), int(width * 0.3), int(height * 0.3)),\n",
    "          \"right_elbow\": (int(width * 0.6), int(height * 0.6), int(width * 0.2), int(height * 0.3)),\n",
    "          \"left_elbow\": (int(width * 0.2), int(height * 0.6), int(width * 0.2), int(height * 0.3))}\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "    exit()\n",
    "# Initialize the tracker with the bounding box\n",
    "\n",
    "tracker_head = cv2.legacy.TrackerMOSSE_create()\n",
    "tracker_right_elbow = cv2.legacy.TrackerMOSSE_create()\n",
    "tracker_left_elbow = cv2.legacy.TrackerMOSSE_create()\n",
    "\n",
    "for joint, rect in joints.items():\n",
    "    x, y, w, h = rect\n",
    "    match joint:\n",
    "        case \"head\":\n",
    "            tracker_head.init(frame, (x, y, w, h))\n",
    "        case \"right_elbow\":\n",
    "            tracker_right_elbow.init(frame, (x, y, w, h))\n",
    "        case \"left_elbow\":\n",
    "            tracker_left_elbow.init(frame, (x, y, w, h))\n",
    "\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=50, detectShadows=False)\n",
    "i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "    if i < 300:\n",
    "        # Show each rectangle in the right position\n",
    "        cv2.rectangle(frame, joints[\"head\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"right_elbow\"], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, joints[\"left_elbow\"], (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "        i += 1\n",
    "        continue\n",
    "    centroids = {}\n",
    "    for joint in joints.keys():\n",
    "        match joint:\n",
    "            case \"head\":\n",
    "                rect = joints[joint]\n",
    "                height, width, _ = frame.shape\n",
    "                ret, bbox = tracker_head.update(frame)\n",
    "                if ret:\n",
    "                    # Tracking success\n",
    "                    x, y, w, h = [int(v) for v in bbox]\n",
    "                    # cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    center_x = x + w // 2\n",
    "                    center_y = y + h // 2\n",
    "                    cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "                    centroids[\"head\"] = (center_x, center_y)\n",
    "                else:\n",
    "                    # Tracking failure\n",
    "                    cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "            case \"right_elbow\":\n",
    "                rect = joints[joint]\n",
    "                height, width, _ = frame.shape\n",
    "                ret, bbox = tracker_right_elbow.update(frame)\n",
    "                if ret:\n",
    "                    # Tracking success\n",
    "                    x, y, w, h = [int(v) for v in bbox]\n",
    "                    # cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    center_x = x + w // 2\n",
    "                    center_y = y + h // 2\n",
    "                    cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "                    centroids[\"right_elbow\"] = (center_x, center_y)\n",
    "                else:\n",
    "                    # Tracking failure\n",
    "                    cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "            case \"left_elbow\":\n",
    "                rect = joints[joint]\n",
    "                height, width, _ = frame.shape\n",
    "                ret, bbox = tracker_left_elbow.update(frame)\n",
    "                if ret:\n",
    "                    # Tracking success\n",
    "                    x, y, w, h = [int(v) for v in bbox]\n",
    "                    # cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    center_x = x + w // 2\n",
    "                    center_y = y + h // 2\n",
    "                    cv2.circle(frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "                    centroids[\"left_elbow\"] = (center_x, center_y)\n",
    "                else:\n",
    "                    # Tracking failure\n",
    "                    cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    \n",
    "    # Connect the centroids with lines\n",
    "    if len(centroids) == 3:\n",
    "        cv2.line(frame, centroids[\"head\"], centroids[\"right_elbow\"], (0, 255, 0), 2)\n",
    "        cv2.line(frame, centroids[\"head\"], centroids[\"left_elbow\"], (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test With Adva Plank Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_PATH = \"/Users/guyperets/Documents/IP_Course/YogaCoach/adav_example_plank.mov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(frame):\n",
    "    # back_sub = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=100, detectShadows=False)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(frame, (3, 3), 0)\n",
    "    canny = cv2.Canny(blurred, 80, 240)\n",
    "    # cbacksub = back_sub.apply(canny)\n",
    "    dilated = cv2.dilate(canny, (3, 3), iterations=15)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros_like(frame)\n",
    "    centroids = []\n",
    "    for cnt in contours:\n",
    "        # Connect the contours\n",
    "        cv2.drawContours(mask, [cnt], -1, (255, 255, 255), cv2.FILLED)\n",
    "        M = cv2.moments(cnt)\n",
    "        if M['m00'] != 0:\n",
    "            cX = int(M['m10'] / M['m00'])\n",
    "            cY = int(M['m01'] / M['m00'])\n",
    "            centroids.append((cX, cY))\n",
    "    return mask, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_purple_yoga_mat(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower_purple = np.array([125, 60, 60])\n",
    "    upper_purple = np.array([150, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_purple, upper_purple)\n",
    "    # Detect the biggest contour and return its bounding box\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    return mask, (x, y, w, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_head(binary_frame): # Highest white pixel in the binary image\n",
    "    # Get the indices of the white pixels\n",
    "    white_pixels = np.argwhere(binary_frame == 255)\n",
    "    if len(white_pixels) == 0:\n",
    "        return None\n",
    "    head_pixel = white_pixels[np.argmin(white_pixels[:, 0])]\n",
    "    # print(f\"Found top white pixel at {head_pixel}\")\n",
    "    return head_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_foot(binary_roi):\n",
    "    black_pixels = np.argwhere(binary_roi == 0)\n",
    "    if len(black_pixels) == 0:\n",
    "        return None\n",
    "    # Get the pixel who's closest to the center of the ROI\n",
    "    center = np.array(binary_roi.shape) // 2\n",
    "    foot_pixel = black_pixels[np.argmin(np.linalg.norm(black_pixels - center, axis=1))]\n",
    "    print(f\"Found top black pixel at {foot_pixel}\")\n",
    "    # Extract the black pixel from the roi\n",
    "    cv2.circle(binary_roi, (foot_pixel[1], foot_pixel[0]), 50, 255, -1)\n",
    "    cv2.imshow(\"Frame\", binary_roi)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == 27 or key == ord('q'):\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "    return foot_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_frames(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = load_frames(CLIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 317]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 314]\n",
      "Found top black pixel at [ 52 309]\n",
      "Found top black pixel at [ 52 309]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 311]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 317]\n",
      "Found top black pixel at [ 50 317]\n",
      "Found top black pixel at [ 50 314]\n",
      "Found top black pixel at [ 50 313]\n",
      "Found top black pixel at [ 50 314]\n",
      "Found top black pixel at [ 50 322]\n",
      "Found top black pixel at [ 50 313]\n",
      "Found top black pixel at [ 50 312]\n",
      "Found top black pixel at [ 50 313]\n",
      "Found top black pixel at [ 50 314]\n",
      "Found top black pixel at [ 50 314]\n",
      "Found top black pixel at [ 50 315]\n",
      "Found top black pixel at [ 52 310]\n",
      "Found top black pixel at [ 50 311]\n",
      "Found top black pixel at [ 50 311]\n",
      "Found top black pixel at [ 50 311]\n",
      "Found top black pixel at [ 50 311]\n",
      "Found top black pixel at [ 50 311]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for frame in frames:\n",
    "    yoga_mat_mask, mat_bbox = detect_purple_yoga_mat(frame)\n",
    "    x, y, w, h = mat_bbox\n",
    "    mask, _ = detect_person(frame)\n",
    "    # Define a ROI in the mat_bbox, and circle every black contour in this ROI\n",
    "    roi = mask[y:y+h, x:x+w]\n",
    "    # Draw the biggest contours on the mask according to a threshold\n",
    "    contours, _ = cv2.findContours(roi, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 100:\n",
    "            # Circle the centroid of the contour\n",
    "            M = cv2.moments(cnt)\n",
    "            if M['m00'] != 0:\n",
    "                cX = int(M['m10'] / M['m00'])\n",
    "                cY = int(M['m01'] / M['m00'])\n",
    "                cv2.circle(mask, (x + cX, y + cY), 5, (255, 0, 0), -1)\n",
    "    head = detect_head(mask)\n",
    "    foot = detect_foot(yoga_mat_mask[y:y+h, x:x+w])\n",
    "    # For foot: Match the ROI pixel coordinates to the frame coordinates\n",
    "    foot = (foot[0] + y, foot[1] + x)\n",
    "    cv2.line(mask, (head[1], head[0]), (foot[1], foot[0]), (255, 0, 0), 2)\n",
    "    mask_and_mat = np.concatenate((mask, yoga_mat_mask), axis=1)\n",
    "    cv2.imshow(\"Frame\", mask_and_mat)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Optical Flow to track joints\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CLIP_PATH = \"/Users/guyperets/Documents/IP_Course/YogaCoach/adav_example_plank.mov\"\n",
    "NUM_JOINTS = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = load_frames(CLIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_coords = []\n",
    "for i, frame in enumerate(frames):\n",
    "    if i == 0: # FIRST FRAME\n",
    "        for _ in range(NUM_JOINTS):\n",
    "            roi = cv2.selectROI(\"Frame\", frame)\n",
    "            cv2.waitKey(1)\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            x, y, w, h = roi\n",
    "            joint_coord = (x + w // 2, y + h // 2) # Center of the ROI\n",
    "            joint_coords.append(joint_coord)\n",
    "        joint_coords = np.array(joint_coords, dtype=np.float32)\n",
    "        prev_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # prev_frame = cv2.GaussianBlur(prev_frame, (3, 3), 0)\n",
    "        # prev_frame = cv2.Canny(prev_frame, 80, 240)\n",
    "        continue\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # dtype is uint8\n",
    "    # frame = cv2.GaussianBlur(frame, (3, 3), 0)\n",
    "    # frame = cv2.Canny(frame, 80, 240)\n",
    "    next_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_frame, frame, joint_coords, None)\n",
    "    status = status.flatten()\n",
    "    good_new = next_pts[status == 1]\n",
    "    good_old = joint_coords[status == 1]\n",
    "\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        # Turn to integers\n",
    "        a, b, c, d = int(a), int(b), int(c), int(d)\n",
    "        cv2.circle(frame, (a, b), 5, (255, 0, 0), -1)\n",
    "        cv2.line(frame, (a, b), (c, d), (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "    prev_frame = frame.copy()\n",
    "        \n",
    "\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING CODE (OPTICAL FLOW CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def detect_person(frame, backSub, persistent_mask):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    roi = frame[int(height * 0.1):int(height * 0.9), int(width * 0.25):int(width * 0.75)]\n",
    "\n",
    "    fg_mask = backSub.apply(roi)\n",
    "\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    edges = cv2.Canny(roi, 50, 150)\n",
    "    combined_mask = cv2.bitwise_or(fg_mask, edges)\n",
    "\n",
    "    # Perform additional morphological operations to connect close components\n",
    "    combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "\n",
    "    # Threshold the mask to remove small noises\n",
    "    _, combined_mask = cv2.threshold(combined_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # If this is the first frame, initialize the persistent mask\n",
    "    if persistent_mask is None:\n",
    "        persistent_mask = combined_mask\n",
    "    else:\n",
    "        # Update the persistent mask selectively\n",
    "        persistent_mask = cv2.addWeighted(persistent_mask, 0.2, combined_mask, 0.8, 0)\n",
    "\n",
    "        # Apply threshold to ensure persistent mask doesn't accumulate too much information\n",
    "        _, persistent_mask = cv2.threshold(persistent_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the persistent mask\n",
    "    contours, _ = cv2.findContours(persistent_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create an empty mask where the contours will be drawn and filled\n",
    "    human_mask = np.zeros_like(persistent_mask)\n",
    "\n",
    "    # Draw and fill the contours on the mask\n",
    "    cv2.drawContours(human_mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Resize the mask back to the original frame size\n",
    "    full_mask = np.zeros_like(frame)\n",
    "    full_mask[int(height * 0.1):int(height * 0.9), int(width * 0.25):int(width * 0.75)] = human_mask\n",
    "\n",
    "    return full_mask, backSub, persistent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "def calibration():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        return\n",
    "    first_frame = cv2.flip(first_frame, 1)\n",
    "    width = first_frame.shape[1]\n",
    "    height = first_frame.shape[0]\n",
    "    w = 70\n",
    "    h = 70\n",
    "    joints = {\"shoulder\": (width // 2 - w // 2, height // 5, w, h), # (x, y, w, h) - xy is the top left corner\n",
    "              \"elbow\": (int(width // 2 + 1 * w), height // 5, w, h),\n",
    "              \"wrist\": (int(width // 2 + 2 * w), height // 5, w, h),\n",
    "              \"hip\": (width // 2 - w // 2, int(height // 5 + 2.75 * h), w, h),\n",
    "              \"knee\": (width // 2 - w // 2, int(height // 5 + 4.5 * h), w, h),\n",
    "              \"ankle\": (width // 2 - w // 2, int(height // 5 + 5.5 * h + 50), w, h)}\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame.\")\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        for _, rect in joints.items():\n",
    "            x, y, w, h = rect\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) # When user is ready (fits his joints in the boxes), press space\n",
    "        if key == 32: # Space ASCII code\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    centers = {joint: (rect[0] + rect[2] // 2, rect[1] + rect[3] // 2) for joint, rect in joints.items()}\n",
    "    # print(f\"Centers Are: {centers}\")\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 14:04:24.194 Python[31216:3329710] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to capture frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "def detect_body_color(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array([0, 48, 80])\n",
    "    upper = np.array([20, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "    return mask\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    body_mask = detect_body_color(frame)\n",
    "    cv2.imshow(\"Frame\", body_mask)\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "# Create Kalman Filters for each point in p0\n",
    "def create_kalman_filters(p0):\n",
    "    kalman_filters = []\n",
    "    for point in p0:\n",
    "        kf = cv.KalmanFilter(4, 2)  # 4 dynamic params (x, y, vx, vy), 2 measurement params (x, y)\n",
    "        kf.transitionMatrix = np.array([[1, 0, 1, 0],\n",
    "                                        [0, 1, 0, 1],\n",
    "                                        [0, 0, 1, 0],\n",
    "                                        [0, 0, 0, 1]], np.float32)\n",
    "        kf.measurementMatrix = np.eye(2, 4, dtype=np.float32)\n",
    "        kf.processNoiseCov = np.eye(4, 4, dtype=np.float32) * 0.03\n",
    "        kf.measurementNoiseCov = np.eye(2, 2, dtype=np.float32) * 1\n",
    "        kf.errorCovPost = np.eye(4, 4, dtype=np.float32) * 0.1\n",
    "        kf.statePost[:2, 0] = point.ravel()  # Initial position\n",
    "        kalman_filters.append(kf)\n",
    "    return kalman_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "JOINTS_LOCATIONS = calibration() # Returns a dictionary with the {joint: joint_coords}\n",
    "#CLIP_PATH = \"/Users/guyperets/Documents/IP_Course/YogaCoach/adva_example_chaturanga.mov\"\n",
    "# frames = load_frames(CLIP_PATH)\n",
    "\n",
    "backSub = cv.createBackgroundSubtractorMOG2(history=100, varThreshold=50, detectShadows=False)\n",
    "persistent_mask = None\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = {\n",
    "             \"winSize\" : (15, 15),\n",
    "             \"maxLevel\": 3,\n",
    "             \"criteria\": (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "             \"minEigThreshold\": 1e-4,\n",
    "             \"flags\": 0\n",
    "            }\n",
    "                  \n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "cap = cv.VideoCapture(0)\n",
    "ret, old_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "    exit()\n",
    "joint_coords = list(JOINTS_LOCATIONS.values())\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "old_gray = cv.GaussianBlur(old_gray, (3, 3), 0)\n",
    "# segmented_old, backSub, persistent_mask = detect_person(old_gray, backSub, persistent_mask)\n",
    "# for _ in range(NUM_JOINTS): # ! REPLACED WITH BOXES CALIBRATION\n",
    "#     roi = cv2.selectROI(\"Frame\", old_frame)\n",
    "#     cv2.waitKey(1)\n",
    "#     cv2.destroyAllWindows()\n",
    "#     cv2.waitKey(1)\n",
    "#     x, y, w, h = roi\n",
    "#     joint_coord = (x + w // 2, y + h // 2) # Center of the ROI\n",
    "#     joint_coords.append(joint_coord)\n",
    "\n",
    "fade_factor = 0.6  # Factor by which to fade the mask in each iteration\n",
    "fade_threshold = 10  # Intensity below which lines will be completely removed\n",
    "\n",
    "p0 = np.array(joint_coords, dtype=np.float32)\n",
    "kfs = create_kalman_filters(p0)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "count = 0\n",
    "hsv = np.zeros_like(old_gray)\n",
    "hsv[..., 1] = 255\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"ERROR: Failed To Capture Frame\")\n",
    "        exit()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv.GaussianBlur(frame_gray, (3, 3), 0)\n",
    "    # segmented_frame, backSub, persistent_mask = detect_person(frame_gray, backSub, persistent_mask)\n",
    "    predicted_points = []\n",
    "    for kf in kfs:\n",
    "        prediction = kf.predict()\n",
    "        predicted_points.append(prediction[:2].reshape(-1))\n",
    "\n",
    "    predicted_points = np.array(predicted_points, dtype=np.float32).reshape(-1, 1, 2)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    if p1 is not None and st is not None:\n",
    "        good_new = []\n",
    "        for i, kf in enumerate(kfs):\n",
    "            if st[i]:\n",
    "                measurement = p1[i].reshape(-1, 1)\n",
    "                kf.correct(measurement)\n",
    "                good_new.append(p1[i].reshape(2))\n",
    "            else:\n",
    "                if count < 100:\n",
    "                    count += 1\n",
    "                    good_new.append(p0[i].reshape(2))\n",
    "                else:\n",
    "                    good_new.append(predicted_points[i].reshape(2))\n",
    "    else:\n",
    "        print(\"ERROR: Optical flow calculation failed\")\n",
    "        p0 = np.array(joint_coords, dtype=np.float32)\n",
    "        old_gray = frame_gray.copy()\n",
    "        continue\n",
    "    good_new = np.array(good_new, dtype=np.float32).reshape(-1, 1, 2)\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, p0)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        a, b, c, d = int(a), int(b), int(c), int(d)\n",
    "        cv.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        cv.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "    mask = cv.multiply(mask, fade_factor)\n",
    "    mask[mask < fade_threshold] = 0\n",
    "    img = cv.add(frame, mask)\n",
    "    connect_relevant_joints(img, good_new)\n",
    "    check_pose(img, good_new)\n",
    "    cv.imshow('frame', img)\n",
    "    k = cv.waitKey(1)\n",
    "    if k == 27 or k == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farneback Optical Flow Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "    exit()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('Failed to capture frame.')\n",
    "        continue\n",
    "    frame2 = cv.flip(frame2, 1)\n",
    "    frame2 = cv.GaussianBlur(frame2, (3, 3), 0)\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    gray = cv.cvtColor(bgr, cv.COLOR_BGR2GRAY)\n",
    "    THRESHOLD = 70\n",
    "    gray[gray < THRESHOLD] = 0\n",
    "    gray[gray >= THRESHOLD] = 255\n",
    "    cv.imshow('frame2', gray)\n",
    "    k = cv.waitKey(1)\n",
    "    if k == 27 or k == ord('q'):\n",
    "        break\n",
    "    prvs = next\n",
    "\n",
    "cv.waitKey(1)\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising The Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to capture frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the first frame to grayscale\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Parameters for visualizing the flow field\n",
    "step = 16  # Step size for drawing arrows\n",
    "h, w = prvs.shape[:2]\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('Failed to capture frame.')\n",
    "        continue\n",
    "\n",
    "    frame2 = cv.flip(frame2, 1)\n",
    "    frame2 = cv.GaussianBlur(frame2, (3, 3), 0)\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate the dense optical flow using Farneback method\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Draw the optical flow field\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            # Get the flow vector at this point\n",
    "            fx, fy = flow[y, x]\n",
    "            # Draw an arrow on the image\n",
    "            cv.arrowedLine(frame2, (x, y), (int(x + fx), int(y + fy)), (0, 255, 0), 2, tipLength=0.3)\n",
    "\n",
    "    # Display the frame with the movement arrows\n",
    "    cv.imshow('Movement Field', frame2)\n",
    "\n",
    "    # Wait for key press\n",
    "    k = cv.waitKey(1)\n",
    "    if k == 27 or k == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Update previous frame\n",
    "    prvs = next\n",
    "\n",
    "# Cleanup\n",
    "cv.waitKey(1)\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINT_NAMES = [\"WRIST\", \"SHOULDER\", \"ELBOW\", \"HIP\", \"KNEE\", \"ANKLE\"]\n",
    "def connect_relevant_joints(img, joints_coords):\n",
    "    # Connect all joints one after another\n",
    "    for i in range(len(joints_coords) - 1):\n",
    "        # Convert to int (pixel coordinates)\n",
    "        joint1 = tuple(map(int, joints_coords[i][0]))\n",
    "        joint2 = tuple(map(int, joints_coords[i + 1][0]))\n",
    "        # Draw the line\n",
    "        cv.line(img, joint1, joint2, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_params = [(\"SHOULDER\", \"ELBOW\", \"WRIST\", 180), (\"SHOULDER\", \"HIP\", \"ANKLE\", 120)]\n",
    "def check_pose(img, joints_coords):\n",
    "    for i, (joint1, joint2, joint3, angle) in enumerate(pose_params):\n",
    "        joint1_idx = JOINT_NAMES.index(joint1)\n",
    "        joint2_idx = JOINT_NAMES.index(joint2)\n",
    "        joint3_idx = JOINT_NAMES.index(joint3)\n",
    "        angle = np.deg2rad(angle)\n",
    "        joint1_coords = joints_coords[joint1_idx].ravel()\n",
    "        joint2_coords = joints_coords[joint2_idx].ravel()\n",
    "        joint3_coords = joints_coords[joint3_idx].ravel()\n",
    "        v1 = joint1_coords - joint2_coords\n",
    "        v2 = joint3_coords - joint2_coords\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle_between_joints = np.arccos(cos_angle)\n",
    "        text_position = (10, 30 * (i * 2 + 1))\n",
    "        if abs(angle_between_joints - angle) < 5:\n",
    "            cv.putText(  # TODO CHANGE TO TTS\n",
    "                img,\n",
    "                f\"Angle between {joint1}, {joint2} and {joint3} is {np.rad2deg(angle_between_joints):.2f} degrees\",\n",
    "                text_position,\n",
    "                cv.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv.LINE_AA,\n",
    "            )\n",
    "        else:\n",
    "            cv.putText(  # TODO CHANGE TO TTS\n",
    "                img,\n",
    "                \"FIX YOUR POSE\",\n",
    "                text_position,\n",
    "                cv.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "                cv.LINE_AA,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ADVA FULL VIDEO\n",
    "# POSE CORRESPONDING FRAME INDEXES ARE\n",
    "PLANK_INDEXES = [90, 120]\n",
    "CHITURANGA_INDEXES = [150, 180]\n",
    "UPWARD_DOG_INDEXES = [200, 260]\n",
    "DOWNWARD_DOG_INDEXES = [330, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "CLIP_PATH = \"/Users/guyperets/Documents/IP_Course/Project/DEMO_FULL.mov\"\n",
    "frames = load_frames(CLIP_PATH)\n",
    "static_frames = 0\n",
    "backSub = cv.createBackgroundSubtractorMOG2(history=100, varThreshold=50, detectShadows=False)\n",
    "persistent_mask = None\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = {\n",
    "             \"winSize\" : (15, 15),\n",
    "             \"maxLevel\": 3,\n",
    "             \"criteria\": (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "             \"minEigThreshold\": 1e-4,\n",
    "             \"flags\": 0\n",
    "            }\n",
    "                  \n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "old_frame = frames[0]\n",
    "curr_frame_idx = 1\n",
    "joint_coords = []\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "old_gray = cv.GaussianBlur(old_gray, (3, 3), 0)\n",
    "for _ in range(6): # ! REPLACED WITH BOXES CALIBRATION\n",
    "    roi = cv2.selectROI(\"Frame\", old_frame)\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    x, y, w, h = roi\n",
    "    joint_coord = (x + w // 2, y + h // 2) # Center of the ROI\n",
    "    joint_coords.append(joint_coord)\n",
    "\n",
    "fade_factor = 0.6  # Factor by which to fade the mask in each iteration\n",
    "fade_threshold = 10  # Intensity below which lines will be completely removed\n",
    "\n",
    "p0 = np.array(joint_coords, dtype=np.float32)\n",
    "kfs = create_kalman_filters(p0)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "count = 0\n",
    "hsv = np.zeros_like(old_gray)\n",
    "hsv[..., 1] = 255\n",
    "while True:\n",
    "    frame = frames[curr_frame_idx]\n",
    "    curr_frame_idx += 1\n",
    "    if curr_frame_idx == len(frames):\n",
    "        break\n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv.GaussianBlur(frame_gray, (3, 3), 0)\n",
    "    # segmented_frame, backSub, persistent_mask = detect_person(frame_gray, backSub, persistent_mask)\n",
    "    predicted_points = []\n",
    "    for kf in kfs:\n",
    "        prediction = kf.predict()\n",
    "        predicted_points.append(prediction[:2].reshape(-1))\n",
    "\n",
    "    predicted_points = np.array(predicted_points, dtype=np.float32).reshape(-1, 1, 2)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    if p1 is not None and st is not None:\n",
    "        good_new = []\n",
    "        for i, kf in enumerate(kfs):\n",
    "            if st[i]:\n",
    "                measurement = p1[i].reshape(-1, 1)\n",
    "                kf.correct(measurement)\n",
    "                good_new.append(p1[i].reshape(2))\n",
    "            else:\n",
    "                if count < 100:\n",
    "                    count += 1\n",
    "                    good_new.append(p0[i].reshape(2))\n",
    "                else:\n",
    "                    good_new.append(predicted_points[i].reshape(2))\n",
    "    else:\n",
    "        print(\"ERROR: Optical flow calculation failed\")\n",
    "        p0 = np.array(joint_coords, dtype=np.float32)\n",
    "        old_gray = frame_gray.copy()\n",
    "        continue\n",
    "    good_new = np.array(good_new, dtype=np.float32).reshape(-1, 1, 2)\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, p0)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        a, b, c, d = int(a), int(b), int(c), int(d)\n",
    "        cv.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        cv.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "    mask = cv.multiply(mask, fade_factor)\n",
    "    mask[mask < fade_threshold] = 0\n",
    "    img = cv.add(frame, mask)\n",
    "    connect_relevant_joints(img, good_new)\n",
    "    # Start checking the pose only after there is no much movement (the pose should static)\n",
    "    if np.all(p1 - p0 <= 2):\n",
    "        static_frames += 1\n",
    "    else:\n",
    "        static_frames = 0\n",
    "    if static_frames > 10:\n",
    "        check_pose(img, good_new)\n",
    "    \n",
    "    cv.putText(img, f\"Frame: {curr_frame_idx}\", (1100, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.imshow('frame', img)\n",
    "    k = cv.waitKey(100)\n",
    "    if k == 27 or k == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cv2.waitKey(1)\n",
    "cv.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layer 1, 2, and 3.\n",
      "Version 0.3.2-1 (2012/03/25). Written and copyrights by Joe Drew,\n",
      "now maintained by Nanakos Chrysostomos and others.\n",
      "Uses code from various people. See 'README' for more!\n",
      "THIS SOFTWARE COMES WITH ABSOLUTELY NO WARRANTY! USE AT YOUR OWN RISK!\n",
      "tcgetattr(): Operation not supported on socket\n",
      "\n",
      "Playing MPEG stream from hello.mp3 ...\n",
      "MPEG 2.0 layer III, 64 kbit/s, 24000 Hz mono\n",
      "                                                                            \n",
      "[0:03] Decoding of hello.mp3 finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Text to convert to speech\n",
    "text = \"Hello, this is a test of Google Text-to-Speech.\"\n",
    "\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text=text, lang='en')\n",
    "\n",
    "# Play the speech\n",
    "tts.save(\"hello.mp3\")\n",
    "os.system(\"mpg321 hello.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse clicked at position: (711, 180)\n",
      "Mouse clicked at position: (395, 529)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo Frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Break the loop if 'q' is pressed\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Release the capture and close all OpenCV windows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def get_pixel_coordinates(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # If the left mouse button was clicked\n",
    "        return x, y\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "    exit()\n",
    "\n",
    "cv2.namedWindow(\"Calibration Frame\")\n",
    "cv2.setMouseCallback(\"Calibration Frame\", get_pixel_coordinates)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video Frame\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last click at: (769, 248)\n",
      "Last click at: (970, 276)\n",
      "Last click at: (897, 407)\n",
      "Last click at: (507, 517)\n",
      "Last click at: (654, 277)\n",
      "Last click at: (805, 362)\n",
      "Last click at: (763, 204)\n",
      "Last click at: (763, 203)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m cv2\u001b[38;5;241m.\u001b[39msetMouseCallback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo Frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, get_pixel_coordinates)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initial text to display\n",
    "text = \"Click anywhere on the screen\"\n",
    "\n",
    "# Callback function to capture mouse events and update text\n",
    "def get_pixel_coordinates(event, x, y, flags, param):\n",
    "    global text\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # If the left mouse button was clicked\n",
    "        text = f\"Last click at: ({x}, {y})\"\n",
    "        print(text)  # Print the coordinates to the console as well\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "    exit()\n",
    "\n",
    "# Set up a named window and bind the mouse callback function to this window\n",
    "cv2.namedWindow(\"Video Frame\")\n",
    "cv2.setMouseCallback(\"Video Frame\", get_pixel_coordinates)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the text on the frame\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Video Frame\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layer 1, 2, and 3.\n",
      "Version 0.3.2-1 (2012/03/25). Written and copyrights by Joe Drew,\n",
      "now maintained by Nanakos Chrysostomos and others.\n",
      "Uses code from various people. See 'README' for more!\n",
      "THIS SOFTWARE COMES WITH ABSOLUTELY NO WARRANTY! USE AT YOUR OWN RISK!\n",
      "tcgetattr(): Operation not supported on socket\n",
      "\n",
      "Playing MPEG stream from hello_faster.mp3 ...\n",
      "MPEG 2.0 layer III, 64 kbit/s, 24000 Hz mono\n",
      "                                                                            \n",
      "[0:02] Decoding of hello_faster.mp3 finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Text to convert to speech\n",
    "text = \"Hello, this is a test of Google Text-to-Speech.\"\n",
    "\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text=text, lang='en')\n",
    "\n",
    "# Save the speech to an MP3 file\n",
    "tts.save(\"hello.mp3\")\n",
    "\n",
    "# Load the MP3 file\n",
    "sound = AudioSegment.from_file(\"hello.mp3\")\n",
    "\n",
    "# Speed up the audio (1.5x)\n",
    "faster_sound = sound.speedup(playback_speed=1.2)\n",
    "\n",
    "# Save the modified file\n",
    "faster_sound.export(\"hello_faster.mp3\", format=\"mp3\")\n",
    "\n",
    "# Play the modified file\n",
    "os.system(\"mpg321 hello_faster.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
